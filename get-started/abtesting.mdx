---
title: "A/B test models"
description: "A/B test selected models on a percentage of live traffic"
---
A/B Tests allow you to evaluate a set of models on a given metric against a percentage of live traffic. When you
[make an inference request](/get-started/inference) for a use case with an active A/B test wihout addressing a specific model,
it is routed to the test with a probability of `traffic split %`. A/B test requests are then distributed equally among tested models.
For example, if your configured `traffic split` is 10% and you are A/B testing 2 models, 5% of the full use case traffic will be routed to each model.

After creating an A/B test, you can start making inference requests and providing feedback on the resulting completions. 
When enough feedback has been logged to reach a statistically significant performance difference between the evaluated modelsâ€™, the A/B test ends automatically.
The test results will show the average value of feedbacks logged for each model.

If you want to bypass the configured `traffic split` and guarantee your request counts towards the A/B test,
you can specify the A/B test key in the `ab_campaign` parameter when using the [Chat API](/get-started/inference).
