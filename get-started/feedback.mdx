---
title: "Submit feedback"
description: "Log your feedback for outputs of different models"
---
You can easily log your feedback about different model's outputs to Adaptive Engine,
so you can later use it to improve your models.

## Supported feedback types
There are two types of feedback you can provide: direct feedback and comparisons. 

When you provide direct feedback, you are logging a value - associated with a metric - attributed to a single completion.
For example, you can log that Llama3.1 8B’s response to your query scored a `CSAT` of 5. 

When you log a comparison, you are logging what was the preferred and dispreferred response between 2 different outputs
to the same prompt. The outputs might have been generated by the same, or different models. Again, this preference
feedback must be associated with a metric. For example, you can log that Llama3.1 8B’s response to your query was more
`truthful` than Mistral 7B’s response.

## Logging direct feedbacks

When you make an inference request, the API response includes a `completion_id` UUID along with the
model’s output (see [Make inference requests](/get-started/inference) to learn more). You must log your feedback
for an output using its completion_id.

<Warning>
  Make sure to use the response’s `completion_id` for logging, not its `id`
</Warning>

You can access the completion_id for a Chat API response as follows:
<CodeGroup>
```python requests
completion_id = response.json()["choices"][0]["completion_id"]
```

```python Adaptive SDK / OpenAI Python
completion_id = response.choices[0].completion_id
```
</CodeGroup>

Then, you can log your direct feedback.

<CodeGroup>
```python requests
import requests

headers = {"Authorization": "Bearer ADAPTIVE_API_KEY"}
payload = { 
  "value": float|bool,
  "metric": "METRIC_KEY",
  "completion_id" : "COMPLETION_ID"
}

response = requests.post(
  url="ADAPTIVE_URL/api/v1/feedback",
  json=payload,
  headers=headers
)
```

```python Adaptive SDK
from adaptive_sdk.types import AddFeedbackRequest

response = client.add_feedback(
	AddFeedbackRequest(
	  value=float|bool,
	  metric="METRIC_KEY",
	  completion_id="COMPLETION_ID"
  )
)
```
</CodeGroup>

If you want to log clarifying textual feedback along with its scalar/boolean value, you can pass it in the `details` input parameter.
See [Direct feedback API reference](/api-reference/feedback/post-apiv1feedback) for more information.


## Logging comparisons
You can log a comparison between two completions that were executed by Adaptive Engine using their completion id’s
, or log a comparison between two strings directly. In the latter case, you are required to log the prompt that was used as well.

<CodeGroup>
```python requests
import requests

headers = {"Authorization": "Bearer ADAPTIVE_API_KEY"}

payload_w_id = { 
  "metric": "METRIC_KEY",
  "preferred_completion" : "PREFERRED_COMPLETION_ID",
  "other_completion": "OTHER_COMPLETION_ID",
}
payload_w_str = {
  "metric": "METRIC_KEY",
  "preferred_completion" : "PREFERRED_COMPLETION_STR",
  "other_completion": "OTHER_COMPLETION_STR",
  "prompt": "PROMPT_STR"
}

response = requests.post(
  url="ADAPTIVE_URL/api/v1/comparison",
  json=payload_w_id|payload_w_str,
  headers=headers
)
```

```python Adaptive SDK
from adaptive_sdk.types import AddComparisonRequest

payload_w_id = AddComparisonRequest(
  metric="METRIC_KEY",
  preferred_completion="PREFERRED_COMPLETION_ID",
  other_completion="OTHER_COMPLETION_ID"
)

payload_w_str = AddComparisonRequest(
  metric="METRIC_KEY",
  preferred_completion="PREFERRED_COMPLETION_ID",
  other_completion="OTHER_COMPLETION_ID"
)

response = client.add_comparison(payload_w_id|payload_w_str)
```
</CodeGroup>