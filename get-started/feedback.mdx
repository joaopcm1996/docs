---
title: 'Submit feedback on completions'
description: 'Log your feedback on completions of different models'
---
You can easily log your feedback about different model's outputs to Adaptive Engine,
so you can later use it to improve your models.

## Supported feedback types
There are two types of feedback you can provide: direct feedback and comparisons. 

When you provide direct feedback, you are logging a value - associated with a metric - attributed to a single completion.
For example, you can log that Llama3.1 8B’s response to your query scored a `CSAT` of 5. 

When you log a comparison, you are logging what was the preferred and dispreferred response between 2 different outputs
to the same prompt. The outputs might have been generated by the same, or different models. Again, this preference
feedback must be associated with a metric. For example, you can log that Llama3.1 8B’s response to your query was more
`truthful` than Mistral 7B’s response.

## Logging direct feedbacks

When you make an inference request, the API response includes a `completion_id` UUID along with the
model’s output (see [Make inference requests](/get-started/inference) to learn more). You must log your feedback
for the output using its completion_id

<Warning>
  Make sure to use the response’s `completion_id` for logging, not its `id`
</Warning>

You can access the completion_id for the first choice in a chat response as follows:
<CodeGroup>
```python requests
completion_id = response.json()["choices"][0]["completion_id"]
```

```python Adaptive SDK / OpenAI Python
completion_id = response.choices[0].completion_id
```
</CodeGroup>


You can tag/label individual requests by passing in a dictionary of key-value pairs as `labels`,
as you can see in the code snippets above. If you use the OpenAI Python library, you have to pass labels
within the `extra_body`. 

Although the OpenAI Python library can be used to make inference requests to your Adaptive deployment,
not all of Adaptive Engine’s input parameters are supported by OpenAI’s library and vice-versa.
See [Chat API Reference](/api-reference/completions/post-apiv1chatcompletions) for a list of supported parameters.

 
Both the Adaptive SDK’s and OpenAI Python library’s responses are Pydantic Models, which help with autocompletion within your editor. 
You can access the model’s response text with:
<CodeGroup>
```python requests
completion_text = response.json()["choices"][0]["message"]["content"]
```

```python Adaptive SDK / OpenAI Python
completion_text = response.choices[0].message.content
```
</CodeGroup>

Then, you can log your direct feedback.

<CodeGroup>
```python requests
import requests

headers = {"Authorization": "Bearer ADAPTIVE_API_KEY"}
payload = { 
  "value": float|bool,
  "metric": "METRIC_KEY",
  "completion_id" : "COMPLETION_ID"
}

response = requests.post(
  url="ADAPTIVE_URL/api/v1/feedback",
  json=payload,
  headers=headers
)
```

```python Adaptive SDK
from adaptive_sdk.types import AddFeedbackRequest

response = client.add_feedback(
	AddFeedbackRequest(
	  value=float|bool,
	  metric="METRIC_KEY",
	  completion_id="COMPLETION_ID"
  )
)
```
</CodeGroup>

If you want to log textual feedback along with its value, you can pass it as the `details` input parameter.
See [Direct feedback API reference](/api-reference/feedback/post-apiv1feedback) for more information.