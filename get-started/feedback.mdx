---
title: "Logging feedback"
description: "Annotate model's completions with your feedback"
---
Adaptive Engine allows you to easily log feedback on completions to monitor and improve your models.

## Logging direct feedbacks
Direct feedback allows you to score a completion with scalar or boolean values. All direct feedbacks must be logged against a `metric`.
For example, the code snippet below logs that Llama3.1 8B’s completion to your prompt scored a `CSAT` of 5. 

When you make an inference request, the API response includes a `completion_id` UUID along with the
model’s output (see [Make inference requests](/get-started/inference) to learn more). You must log your feedback
for an output using its completion_id.

<Warning>
  Make sure to use the response’s `completion_id` for logging, not its `id`
</Warning>

You can access the completion_id for a Chat API response as follows:
<CodeGroup>
```python requests
completion_id = response.json()["choices"][0]["completion_id"]
```

```python Adaptive SDK / OpenAI Python
completion_id = response.choices[0].completion_id
```
</CodeGroup>

Then, you can log your direct feedback.

<CodeGroup>
```python requests
import requests

headers = {"Authorization": "Bearer ADAPTIVE_API_KEY"}
payload = { 
  "value": 5,
  "metric": "CSAT",
  "completion_id" : "COMPLETION_ID"
}

response = requests.post(
  url="ADAPTIVE_URL/api/v1/feedback",
  json=payload,
  headers=headers
)
```

```python Adaptive SDK
from adaptive_sdk.types import AddFeedbackRequest

response = client.add_feedback(
	AddFeedbackRequest(
	  value=5,
	  metric="CSAT",
	  completion_id="COMPLETION_ID"
  )
)
```
</CodeGroup>

If you want to log clarifying textual feedback along with its scalar/boolean value, you can pass it in the `details` input parameter.
See [Direct feedback API reference](/api-reference/feedback/post-apiv1feedback) for more information.
